---
title: "Capstone"
author: "Rosemarie Portelli"
date: "27/05/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

#Install and load required packages
```{r}
# install.packages("tm")
# install.packages("text2vec")
# install.packages("Snowballc")
# install.packages("wordcloud")
# install.packages("RColorBrewer")
# install.packages("qdapTools")
# install.packages("tidyverse")
# install.packages("tidytext")
# install.packages("ggplot2")
# install.packages("ggthemes")
# install.packages("qdap")
# install.packages("dplyr")
# install.packages("plotrix")
# install.packages("dendextend")
# install.packages("reshape2")
# install.packages("quanteda")
# install.packages("irlba")
# install.packages("e1071")
# install.packages("caret")
# install.packages("randomForest")
# install.packages("rpart")
# install.packages("rpart.plot")
# install.packages("RColorBrewer")
# install.packages("biclust")
# install.packages("igraph")
# install.packages("fpc")
library(tm)
library(text2vec)
library(SnowballC)
library(stopwords)
library(wordcloud)
library(qdapTools)
library(tidyverse)
library(ggplot2)
library(tidytext)
library(dplyr)
library(tm)
library(plotrix)
library(dendextend)
library(ggthemes)
library(reshape2)
library(quanteda)
library(irlba)
library(e1071)
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(biclust)
library(igraph)
library(fpc)
library(stringr)
library(stringi)
library(gplots)
```


## Reading the Transcripts
```{r}
userReview <- read.csv(file = 'C:\\Users\\rosep\\Documents\\CKME136\\user_reviews.csv', header = T, sep = ',', stringsAsFactors = FALSE)

userReviewHigh <- subset(userReview, userReview$indicator == 1)
userReviewLow <- subset(userReview, userReview$indicator == 0)

head(userReview)
```
# Histogram of the rating
```{r}
hist(userReview$grade, main="User Rating", xlab="Grade")
hist(userReviewHigh$grade, main="High User Rating", xlab="High Grade")
hist(userReviewLow$grade, main="Low User Rating", xlab="Low Grade")
```
# Boxplots of the rating
```{r}
boxplot(userReview$grade, main="User Rating", xlab="Grade")
boxplot(userReviewHigh$grade, main="High User Rating", xlab="High Grade")
boxplot(userReviewLow$grade, main="Low User Rating", xlab="Low Grade")
```


# Reading the transcripts
```{r}
userReview <- within(userReview, grade[grade < 5] <- 0)
userReview <- within(userReview, grade[grade > 4] <- 1)
userReview <- subset(userReview, select = -c(user_name, date, indicator))

userDoc <- 0
for (i in c(2:3000)) {userDoc[i] <- as.character(userReview$text[i])}
Udoc.list <- as.list(userDoc[2:3000])
N.docs <- length(Udoc.list)
names(Udoc.list) <- paste0("UDoc", c(1:N.docs))
Query <- as.character(userReview$text[1])
```

#Preparing the Corpus
```{r}
my.Udocs <- VectorSource(c(Udoc.list, Query))
my.Udocs$Names <- c(names(Udoc.list), "Query")
my.Ucorpus <- Corpus(my.Udocs)
my.Ucorpus

UcorpusHigh <- Corpus(VectorSource(userReviewHigh))
UcorpusLow <- Corpus(VectorSource(userReviewLow))
```

#Cleaning and Preprocessing the text
```{r}
getTransformations()

#Change special characters to space
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
my.Ucorpus <- tm_map(my.Ucorpus, content_transformer(function(x) gsub(x, pattern = "@.*", replacement = "")))
#Removing punctuation is necessary as it helps to increase retrieval performance
my.Ucorpus <- tm_map(my.Ucorpus, removePunctuation)
#Easy to compare the words in the documents to the query when all the words have been transformed to lower case
my.Ucorpus <- tm_map(my.Ucorpus, content_transformer(tolower))
#stopwords are commonly used words that seldom contribute to the meaning of the sentence. They can interfere with the precision and recall
#my.Ucorpus <- stopwords(my.Ucorpus="en")
my.Ucorpus <- tm_map(my.Ucorpus, removeWords, stopwords("english"))
my.Ucorpus <- tm_map(my.Ucorpus, removeWords, c("nintendo", "animal", "crossing", "new", "horizons", "switch", "game", "TMS"))
#Stemming reduces the different forms of the word formed by inflections and derivation to a common stem
my.Ucorpus <- tm_map(my.Ucorpus, stemDocument)
#Remove extra white spaces
my.Ucorpus <- tm_map(my.Ucorpus, stripWhitespace)
content(my.Ucorpus[[1]])

UcorpusHigh <- tm_map(UcorpusHigh, content_transformer(function(x) gsub(x, pattern = "@.*", replacement = "")))
#UcorpusHigh <- tm_map(UcorpusHigh, toSpace, c("â€", "€_™_s"))
UcorpusHigh <- tm_map(UcorpusHigh, removePunctuation)
UcorpusHigh <- tm_map(UcorpusHigh, content_transformer(tolower))
UcorpusHigh <- tm_map(UcorpusHigh, removeWords, stopwords("english"))
UcorpusHigh <- tm_map(UcorpusHigh, removeWords, c("nintendo", "animal", "crossing", "new", "horizons", "horizon", "switch", "game"))
UcorpusHigh <- tm_map(UcorpusHigh, stemDocument)
UcorpusHigh <- tm_map(UcorpusHigh, stripWhitespace)

UcorpusLow <- tm_map(UcorpusLow, content_transformer(function(x) gsub(x, pattern = "@.*", replacement = "")))
#UcorpusLow <- tm_map(UcorpusLow, toSpace, "â€")
UcorpusLow <- tm_map(UcorpusLow, removePunctuation)
UcorpusLow <- tm_map(UcorpusLow, content_transformer(tolower))
UcorpusLow <- tm_map(UcorpusLow, removeWords, stopwords("english"))
UcorpusLow <- tm_map(UcorpusLow, removeWords, c("nintendo", "animal", "crossing", "new", "horizons", "horizon", "switch", "game"))
UcorpusLow <- tm_map(UcorpusLow, stemDocument)
UcorpusLow <- tm_map(UcorpusLow, stripWhitespace)
```

##Creating a uni-gram Term Document Matrix and Document Term Matrix
```{r}
Uterm.doc.matrix <- TermDocumentMatrix(my.Ucorpus)
inspect(Uterm.doc.matrix[1:10,1:10])

Udoc.term.matrix <- DocumentTermMatrix(my.Ucorpus)
inspect(Udoc.term.matrix)

UHigh.TDM <- TermDocumentMatrix(UcorpusHigh)
ULow.TDM <- TermDocumentMatrix(UcorpusLow)
```

## Converting the generated TDM into a matrix
```{r}
User.TDM <- as.matrix(Uterm.doc.matrix)
#head(User.TDM)
#dim(User.TDM)

UHigh.TDM <- as.matrix(UHigh.TDM)
ULow.TDM <- as.matrix(ULow.TDM)
```

#Sorting the matrix and checking frequency
```{r}
UsortTDMatrix <- sort(rowSums(User.TDM),decreasing=TRUE)
UdMatrix <- data.frame(word = names(UsortTDMatrix),freq=UsortTDMatrix)
head(UdMatrix, 10)

UsortHTDMatrix <- sort(rowSums(UHigh.TDM),decreasing=TRUE)
UHTDMatrix <- data.frame(word = names(UsortHTDMatrix),freq=UsortHTDMatrix)

UsortLTDMatrix <- sort(rowSums(ULow.TDM),decreasing=TRUE)
ULTDMatrix <- data.frame(word = names(UsortLTDMatrix),freq=UsortLTDMatrix)
```

#Creating the word cloud
```{r}
set.seed(1234)
wordcloud(words = UdMatrix$word, freq = UdMatrix$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
barplot(UdMatrix[1:10,]$freq, las=2, names.arg=UdMatrix[1:10,]$word, main="Commonly occurring words")
```

#Comparing the frequencies 
```{r}
barplot(UHTDMatrix[1:10,]$freq, las=2, names.arg=UHTDMatrix[1:10,]$word, main="Commonly occurring words in High Grade")
barplot(ULTDMatrix[1:10,]$freq, las=2, names.arg=ULTDMatrix[1:10,]$word, main="Commonly occurring words in Low Grade")
```

#Declaring weights (TF-IDF)
```{r}
get.tf.idf.weights <- function(tf.vec) {
  # Computes the tfidf weights from the term frequency vector
  n.docs <- length(tf.vec)
  doc.frequency <- length(tf.vec[tf.vec > 0])
  weights <- rep(0, length(tf.vec))
  relative.frequency <- tf.vec[tf.vec > 0] / sum(tf.vec[tf.vec > 0])
  weights[tf.vec > 0] <-  relative.frequency * log(n.docs/doc.frequency)
  return(weights)
}
```



###Utfidf.matrix is the transposed version of term.doc.matrix
```{r}
Utfidf.matrix <- t(apply(Uterm.doc.matrix, 1,
                        FUN = function(row) {get.tf.idf.weights(row)})) #apply the functions against the rows

colnames(Utfidf.matrix) <- my.Udocs$Names

Utfidf.t <- t(Utfidf.matrix)

Utfidf.t.df <- as.dfm(Utfidf.t)

#head(Utfidf.matrix)
#dim(Utfidf.matrix)
```

#computing the cosine similarity matrix
```{r}
similarity.matrix <- sim2(Utfidf.matrix, method = 'cosine')
heatmap(similarity.matrix[1:20, 1:20], scale = "column")

```




##Creating a dendrogram
```{r}
Uterm.doc.matrix2 <- removeSparseTerms(Uterm.doc.matrix, sparse = 0.9)
hc <- hclust(d = dist(Uterm.doc.matrix2, method = "euclidean"), method = "complete")
# Plot a dendrogram
plot(hc)
```

## Create associations
```{r}
associations <- findAssocs(Uterm.doc.matrix2[1:20,], "island", 0.05)
# Create associations_df
associations_df <- list_vect2df(associations)[, 2:3]
# Plot the associations_df values 
ggplot(associations_df, aes(y = associations_df[, 1])) + 
  geom_point(aes(x = associations_df[, 2]), 
             data = associations_df, size = 3) + 
  ggtitle("Word Associations to 'island'") + 
  theme_gdocs()
```

##Creating bi-grams and tri-grams
```{r}
rmwords <- c("nintendo", "switch", "new", "horizons", "horizon", "animal", "crossing")

##create bi-grams
review_bigram <- tokens(userReview$text) %>%
  tokens_remove("[^[:alnum:]]", valuetype = "regex", padding = TRUE) %>%
  tokens_remove("[^a-zA-Z0-9]", valuetype = "regex", padding = TRUE) %>%
  tokens_select(rmwords, selection = "remove", padding = TRUE) %>%
  tokens_remove(stopwords("english"), padding  = TRUE) %>%
  tokens_ngrams(n = 2) %>%
  dfm()
topfeatures(review_bigram)

##Create tri-grams
review_trigram <- tokens(userReview$text) %>%
  tokens_remove("[^[:alnum:]]", valuetype = "regex", padding = TRUE) %>%
  tokens_remove("[^a-zA-Z0-9]", valuetype = "regex", padding = TRUE) %>%
  tokens_select(rmwords, selection = "remove", padding = TRUE) %>%
  tokens_remove(stopwords("english"), padding  = TRUE) %>%
  tokens_ngrams(n = 3) %>%
  dfm()
topfeatures(review_trigram)
```


##Tokenisation
```{r}
## Tokenize descriptions
reviewtokens <- tokens(userReview$text,what="word",
                    remove_numbers=TRUE,remove_punct=TRUE, remove_symbols=TRUE, split_hyphens=TRUE)
# Lowercase the tokens
reviewtokens <- tokens_tolower(reviewtokens)
# remove stop words and unnecessary words
rmwords <- c("nintendo", "switch", "new", "horizons", "horizon", "animal", "crossing")
reviewtokens <- tokens_select(reviewtokens, stopwords(), selection = "remove")
reviewtokens <- tokens_remove(reviewtokens,rmwords)
# Stemming tokens
reviewtokens <- tokens_wordstem(reviewtokens,language = "english")
reviewtokens <- tokens_ngrams(reviewtokens,n=1:3)

# Creating a bag of words
reviewtokensdfm=dfm(reviewtokens,tolower = FALSE)
# Remove sparsity
reviewSparse <- convert(reviewtokensdfm, "tm")
tm::removeSparseTerms(reviewSparse, 0.7)
# Create the dfm
dfm_trim(reviewtokensdfm, min_docfreq = 0.3)
x <- dfm_trim(reviewtokensdfm, sparsity = 0.98)
```

##Classification Model
```{r}
## Setup a dataframe with features
df <- convert(x,to="data.frame")
##Add the Y variable Recommend.IND
reviewtokensdf <- cbind(userReview$grade,df)
head(reviewtokensdf)
## Cleanup names
names(reviewtokensdf)[names(reviewtokensdf) == "userReview$grade"] <- "recommend"
names(reviewtokensdf) <- make.names(names(reviewtokensdf))
head(reviewtokensdf)
## Remove the original review.text column
reviewtokensdf <- reviewtokensdf[,-c(2)]
head(reviewtokensdf)
reviewtokensdf$recommend <- factor(reviewtokensdf$recommend)
```


## Build the CART model
```{r}
library("rpart", "rpart.plot")
tree <- rpart(formula = recommend ~ ., data = reviewtokensdf, method="class",
           control = rpart.control(minsplit = 200,  minbucket = 30, cp = 0.0001))
printcp(tree)
plotcp(tree)


##Prune down the tree
bestcp <- tree$cptable[which.min(tree$cptable[,"xerror"]),"CP"]
bestcp
ptree <- prune(tree,cp=bestcp)
rpart.plot(ptree,cex = 0.6)
prp(ptree, faclen = 0, cex = 0.5, extra = 2)
```

## Random Forest
```{r}
library(randomForest)
reviewRF <- randomForest(recommend ~ ., data = reviewtokensdf)
varImpPlot(reviewRF, cex=.7)
```

## Lasso logistic regression
```{r}
#load required library
library(glmnet)
#dumy code categorical predictor variables as matrix
x <- model.matrix(recommend~.,reviewtokensdf)
#convert class to numerical variable
y <- as.numeric(reviewtokensdf$recommend)
#perform grid search to find optimal value of lambda
cv.llg <- cv.glmnet(x,y,alpha=1,family="binomial")
#plot result
plot(cv.llg)
```

```{r}
#the exact value of lambda
lambda_min <- cv.llg$lambda.min
lambda_min
#best value of lambda
lambda_1se <- cv.llg$lambda.1se
lambda_1se

#regression coefficients
coef <- coef(cv.llg,s=lambda_1se)
lassocoef <- as.matrix(coef(cv.llg,s=lambda_1se))
write.csv(lassocoef, "lasso_coef.csv")

# Find the best lambda using cross-validation
set.seed(123) 
cv.llg <- cv.glmnet(x, y, alpha = 1, family = "binomial")

# Fit the final model on the dataframe
review_logreg <- glmnet(x, y, alpha = 1, family = "binomial",
                        lambda = cv.llg$lambda.min)

# Save the regression coef to a csv file
logregcoef <- as.matrix(coef(review_logreg))
odds_ratio <- as.matrix(exp(coef(review_logreg)))
write.csv(logregcoef, "logreg_coef.csv")
write.csv(odds_ratio, "odds_ratio.csv")
```